{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1. Explain the Activation Functions in your own language\n",
    "\n",
    "a)\tsigmoid\n",
    "\n",
    "The main reason why we use sigmoid function is because it exists between (0 to 1). Therefore, it is especially used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n",
    "\n",
    "b)\ttanh\n",
    "\n",
    "The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1.\n",
    "\n",
    "c)\tReLU\n",
    "\n",
    "The rectified linear activation function or ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero. ... The rectified linear activation function overcomes the vanishing gradient problem, allowing models to learn faster and perform better\n",
    "\n",
    "d)\tELU\n",
    "\n",
    "ELU. Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number. ELU is very similiar to RELU except negative inputs\n",
    "\n",
    "e)\tLeakyReLU\n",
    "\n",
    "Leaky Rectified Linear Unit, or Leaky ReLU, is a type of activation function based on a ReLU, but it has a small slope for negative values instead of a flat slope. ... This type of activation function is popular in tasks where we we may suffer from sparse gradients, for example training generative adversarial networks.\n",
    "\n",
    "f)\tswish\n",
    "\n",
    "Swish is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks applied to a variety of challenging domains such as Image classification and Machine translation. It is unbounded above and bounded below & it is the non-monotonic attribute that actually creates the difference.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a99fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2. What happens when you increase or decrease the optimizer learning rate?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfdd4c",
   "metadata": {},
   "source": [
    "The learning rate hyperparameter controls the rate or speed at which the model learns. Specifically, it controls the amount of apportioned error that the weights of the model are updated with each time they are updated, such as at the end of each batch of training examples. The learning rate controls how quickly the model is adapted to the problem. Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2de47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3. What happens when you increase the number of internal hidden neurons?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e1d5f",
   "metadata": {},
   "source": [
    "An inordinately large number of neurons in the hidden layers can increase the time it takes to train the network. The amount of training time can increase to the point that it is impossible to adequately train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f7a3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4. What happens when you increase the size of batch computation?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030178b3",
   "metadata": {},
   "source": [
    "Training loss and accuracy when the model is trained using different batch sizes. Testing loss and accuracy when the model is trained using different batch sizes. Finding: higher batch sizes leads to lower asymptotic test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db478a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5. Why we adopt regularization to avoid overfitting?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8f0f1c",
   "metadata": {},
   "source": [
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26300761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6. What are loss and cost functions in deep learning?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935537ec",
   "metadata": {},
   "source": [
    "Loss function: Used when we refer to the error for a single training example. Cost function: Used to refer to an average of the loss functions over an entire training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aac2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7. What do ou mean by underfitting in neural networks?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3d744",
   "metadata": {},
   "source": [
    "A model is said to be underfitting when it's not able to classify the data it was trained on. We can tell that a model is underfitting when the metrics given for the training data are poor, meaning that the training accuracy of the model is low and/or the training loss is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617ec49e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "092ed0a0",
   "metadata": {},
   "source": [
    "### 8. Why we use Dropout in Neural Networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eea9f8",
   "metadata": {},
   "source": [
    "A Simple Way to Prevent Neural Networks from Overfitting, 2014. Because the outputs of a layer under dropout are randomly subsampled, it has the effect of reducing the capacity or thinning the network during training. As such, a wider network, e.g. more nodes, may be required when using dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
