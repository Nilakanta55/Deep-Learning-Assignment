{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1.\tWhat are the advantages of a CNN over a fully connected DNN for image classification?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307eb4c5",
   "metadata": {},
   "source": [
    "Fewer parameters -> faster to train\n",
    "Reuse kernel -> detect feature anywhere\n",
    "Architecture embeds knowledge of neighboring pixels\n",
    "\n",
    "Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
    "When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
    "Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN's architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa7f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2.\tConsider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about when training on a mini-batch of 50 images?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7bce26",
   "metadata": {},
   "source": [
    "parameters\n",
    "\n",
    "first convolutional layer kernel-size and RGB channels, plus bias: 3 * 3 * 3 + 1 = 28 output feature maps is 100: 28 * 100 = 2800\n",
    "second convolutional layer kernel-size and last feature maps, plus bias: 3 * 3 * 100 + 1 = 901 output feature maps is 200: 901 * 200 = 180200\n",
    "third convolutional layers kernel-size and last feautre maps, plus bias: 3 * 3 * 200 + 1 =1801 output feautre maps is 400: 1801 * 400 = 720400\n",
    "Total parameters is 2800 + 180200 + 720400 = 903400\n",
    "\n",
    "memories since 32-bit is 4 bytes\n",
    "\n",
    "first convolutional layer one feature map size: 100 * 150 = 15000 total output: 15000 * 100 = 1,500,000\n",
    "second convolutional layer one feature map size: 50 * 75 = 3,750 total output: 3750 * 200 = 750,000\n",
    "third convolutional layer one feature map size: 25 * 38 = 950 total ouput: 950 * 400 = 380, 000\n",
    "(1,500,000 + 750,000 + 380,000) * 4 / 1024 /1024 = 10.032 (MB) 903400 * 4 / 1024 / 1024 = 3.44 (MB) 10.032+ 3.44=13.47(MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e75066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3.\tIf your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f9014",
   "metadata": {},
   "source": [
    "Reduce the mini-batch size.\n",
    "Reduce dimensionality using a larger stride in one or more layers.\n",
    "Remove one or more layers.\n",
    "Use 16-bit floats instead of 32-bit floats.\n",
    "Distribute the CNN across multiple devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976764d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4.\tWhy would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044bed6",
   "metadata": {},
   "source": [
    "A max pooling layer has no parameters at all, whereas a convolutional layer has a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840642a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5.\tWhen would you want to add a local response normalization layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4086e8",
   "metadata": {},
   "source": [
    "This form of normalization makes the neurons that most strongly activate inhibit neurons at the same location but in neighboring feature maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing them to explore a wider range of features, ultimately improving generalization.\n",
    "It is typically used in the lower layers to have a larger pool of low-level features that the upper layers can build upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30ebee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6.\tCan you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet, ResNet, SENet, and Xception?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4455b03",
   "metadata": {},
   "source": [
    "The main innovations in AlexNet compared to LeNet-5 are (1) it is much larger and deeper, and (2) it stacks convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer.\n",
    "\n",
    "The main innovation in GoogLeNet is the introduction of inception modules, which make it possible to have a much deeper net than previous CNN architectures, with fewer parameters.\n",
    "\n",
    "Finally, ResNet's main innovation is the introduction of skip connections, which make it possible to go well beyond 100 layers. Arguably, its simplicity and consistency are also rather innovative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf6f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7.\tWhat is a fully convolutional network? How can you convert a dense layer into a convolutional layer?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2b680f",
   "metadata": {},
   "source": [
    "The fully convolutional network first uses a CNN to extract image features, then transforms the number of channels into the number of classes via a 1×1 convolutional layer, and finally transforms the height and width of the feature maps to those of the input image via the transposed convolution. In a fully convolutional network, we can use upsampling of bilinear interpolation to initialize the transposed convolutional layer.\n",
    "\n",
    "A fully convolution network can be built by simply replacing the FC layers with there equivalent Conv layers. In the example of VGG16 we can do so by first removing the last four layers. One way to do so is to pop layers from the model. In the model stack, each popping will remove the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4e383",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "092ed0a0",
   "metadata": {},
   "source": [
    "### 8.\tWhat is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc549a5c",
   "metadata": {},
   "source": [
    "Semantic Segmentation is a technique that enables us to differentiate different objects in an image. It can be considered an image classification task at a pixel level.\n",
    "\n",
    "semantic segmentation also has a major problem specific difficulty. This difficulty is caused by an ambiguity of boundaries in image space, especially for thin objects such as poles, similar looking objects such as a road and a sidewalk and far away objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9774b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c42e6ada",
   "metadata": {},
   "source": [
    "### 9.\tBuild your own CNN from scratch and try to achieve the highest possible accuracy on MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07734f",
   "metadata": {},
   "source": [
    "The MNIST handwritten digit classification problem is a standard dataset used in computer vision and deep learning.\n",
    "\n",
    "Although the dataset is effectively solved, it can be used as the basis for learning and practicing how to develop, evaluate, and use convolutional deep learning neural networks for image classification from scratch. This includes how to develop a robust test harness for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.\n",
    "\n",
    "In this tutorial, you will discover how to develop a convolutional neural network for handwritten digit classification from scratch.\n",
    "\n",
    "After completing this tutorial, you will know:\n",
    "\n",
    "How to develop a test harness to develop a robust evaluation of a model and establish a baseline of performance for a classification task.\n",
    "How to explore extensions to a baseline model to improve learning and model capacity.\n",
    "How to develop a finalized model, evaluate the performance of the final model, and use it to make predictions on new images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa786b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of loading the mnist dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "# load dataset\n",
    "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
    "# summarize loaded dataset\n",
    "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
    "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "\t# define subplot\n",
    "\tplt.subplot(330 + 1 + i)\n",
    "\t# plot raw pixel data\n",
    "\tplt.imshow(trainX[i], cmap=plt.get_cmap('gray'))\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63042d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "768183fa",
   "metadata": {},
   "source": [
    "### 10.\tUse transfer learning for large image classification, going through these steps:\n",
    "\n",
    "a.\tCreate a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "\n",
    "b.\tSplit it into a training set, a validation set, and a test set.\n",
    "\n",
    "c.\tBuild the input pipeline, including the appropriate preprocessing operations, and optionally add \n",
    "data augmentation.\n",
    "\n",
    "d.\tFine-tune a pretrained model on this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c260878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfa6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_photo/\n",
    "  beach/\n",
    "  dandelion/\n",
    "  mountain/\n",
    "  sunflowers/\n",
    "  city/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d451c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountain= list(data_dir.glob('mountain/*'))\n",
    "PIL.Image.open(str(mountain[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0842d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PIL.Image.open(str(mountain[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
