{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e20457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1.\tDeep Learning.\n",
    "a.\tBuild a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 100\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 100\n",
    "n_hidden4 = 100\n",
    "n_hidden5 = 100\n",
    "n_outputs = 5\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0018f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eda887",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden1 = dense_layer(X, n_hidden1, name='hidden1')\n",
    "    hidden2 = dense_layer(hidden1, n_hidden2, name='hidden2')\n",
    "    hidden3 = dense_layer(hidden2, n_hidden3, name='hidden3')\n",
    "    hidden4 = dense_layer(hidden3, n_hidden4, name='hidden4')\n",
    "    hidden5 = dense_layer(hidden4, n_hidden5, name='hidden5')\n",
    "    logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
    "    \n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be36a1",
   "metadata": {},
   "source": [
    "b.\tUsing Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ccd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3948618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.train.images[mnist.train.labels < 5]\n",
    "y_train = mnist.train.labels[mnist.train.labels < 5]\n",
    "X_test = mnist.test.images[mnist.test.labels < 5]\n",
    "y_test = mnist.test.labels[mnist.test.labels < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67c85f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_split(X, y, n_batches):\n",
    "    np.random.seed(seed=42)\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    for i_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch = X[i_idx]\n",
    "        y_batch = y[i_idx]\n",
    "        yield X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 50\n",
    "n_batches = len(X_train) // batch_size\n",
    "best_loss = float('inf')\n",
    "patience = 2\n",
    "cnt_patience = 0\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_split(X_train, y_train, n_batches):\n",
    "            sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "        loss_test = loss.eval(feed_dict={X: X_test, y: y_test})\n",
    "        print(epoch, 'loss', loss_test, 'accuracy_train:', accuracy_train, 'accuracy_test:', accuracy_test)\n",
    "        if loss_test < best_loss:\n",
    "            best_loss = loss_test\n",
    "        else:\n",
    "            cnt_patience += 1\n",
    "            if cnt_patience > patience:\n",
    "                'Early stopping!'\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb48db4",
   "metadata": {},
   "source": [
    "c.\tTune the hyperparameters using cross-validation and see what precision you can achieve.\n",
    "d.\tNow try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
    "e.\tIs the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d21dddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DnnClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 batch_size=50, \n",
    "                 n_neuron=100):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.n_neuron = n_neuron\n",
    "        \n",
    "    def reset_graph(self, seed=42):\n",
    "        tf.reset_default_graph()\n",
    "        tf.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \n",
    "        self.reset_graph()\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "        y = tf.placeholder(tf.int64, shape=(None), name='y')\n",
    "        \n",
    "        he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "        \n",
    "        dense_layer = partial(tf.layers.dense, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "with tf.name_scope('dnn'):\n",
    "            hidden1 = dense_layer(X, self.n_neuron, name='hidden1')\n",
    "            hidden2 = dense_layer(hidden1, self.n_neuron, name='hidden2')\n",
    "            hidden3 = dense_layer(hidden2, self.n_neuron, name='hidden3')\n",
    "            hidden4 = dense_layer(hidden3, self.n_neuron, name='hidden4')\n",
    "            hidden5 = dense_layer(hidden4, self.n_neuron, name='hidden5')\n",
    "            logits = dense_layer(hidden5, n_outputs, activation=None, name='outputs')\n",
    "            \n",
    "        with tf.name_scope('softmax'):\n",
    "            y_proba = tf.nn.softmax(logits, axis=1, name='y_proba')\n",
    "    \n",
    "        with tf.name_scope('loss'):\n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "            loss = tf.reduce_mean(xentropy, name='loss')\n",
    "        \n",
    "        learning_rate = 0.001\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "            training_op = optimizer.minimize(loss)\n",
    "            \n",
    "        with tf.name_scope('eval'):\n",
    "            correct = tf.nn.in_top_k(logits, y, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "            \n",
    "        self._training_op = training_op\n",
    "        self._accuracy = accuracy\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._logits = logits\n",
    "        self._y_proba = y_proba\n",
    "         def shuffle_split(self, X, y, n_batches):\n",
    "        np.random.seed(seed=42)\n",
    "        rnd_idx = np.random.permutation(len(X))\n",
    "        for i_idx in np.array_split(rnd_idx, n_batches):\n",
    "            X_batch = X[i_idx]\n",
    "            y_batch = y[i_idx]\n",
    "            yield X_batch, y_batch\n",
    "        \n",
    "    def fit(self, X, y, n_epochs=5, X_valid=None, y_valid=None):\n",
    "        self.n_batches = len(X) // self.batch_size\n",
    "        \n",
    "        n_inputs = X.shape[1]\n",
    "        self.n_outputs = len(np.unique(y))\n",
    "        self.classes_ = np.unique(y)\n",
    "        \n",
    "        self._build_graph(n_inputs, self.n_outputs)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        self._session = tf.Session()\n",
    "        with self._session.as_default() as sess:\n",
    "            init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                for X_batch, y_batch in self.shuffle_split(X, y, self.n_batches):\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    sess.run([self._training_op], feed_dict=feed_dict)\n",
    "                accuracy_train = self._accuracy.eval(feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    accuracy_valid = self._accuracy.eval(feed_dict={self._X: X_valid, self._y: y_valid})\n",
    "                    print('epoch:', epoch, 'accuracy_valid:', accuracy_valid*100)\n",
    "                else:\n",
    "                    print('epoch:', epoch, 'accuracy_train:', accuracy_train*100)\n",
    "                \n",
    "                \n",
    "        return self\n",
    "    \n",
    "     def predict_proba(self, X):\n",
    "        with self._session.as_default() as sess:\n",
    "            y_proba = sess.run([self._y_proba], feed_dict={self._X: X})\n",
    "            return np.array(y_proba).reshape((len(X), self.n_outputs))\n",
    "        \n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59356b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('/tmp/data/')\n",
    "\n",
    "X_train = mnist.train.images\n",
    "y_train = mnist.train.labels\n",
    "X_valid = mnist.test.images\n",
    "y_valid = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec9ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = DnnClassifier()\n",
    "dnn_clf.fit(X_train, y_train, 20, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "params_grid = [{'batch_size': [20, 50]}, {'n_neuron': [50, 100]}]\n",
    "gs = GridSearchCV(DnnClassifier(), params_grid, \n",
    "                 fit_params={'n_epochs': 20, 'X_valid': X_valid, 'y_valid': y_valid})\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c543dd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2.\tTransfer learning.\n",
    "a.\tCreate a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
    "b.\tTrain this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
    "c.\tTry caching the frozen layers, and train the model again: how much faster is it now?\n",
    "d.\tTry again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
    "e.\tNow unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc948951",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7b3af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a284370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124dd60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b97045a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3af79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205793b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3.\tPretraining on an auxiliary task.\n",
    "a.\tIn this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (let’s call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. To do this, you should use TensorFlow’s concat() function with axis=1 to concatenate the outputs of both DNNs for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function.\n",
    "b.\tSplit the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
    "c.\tTrain the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
    "d.\tNow create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3aa235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011c533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6f11e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
