{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1.\tIs it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a1439",
   "metadata": {},
   "source": [
    "It makes the hidden units symmetric and this problem is known as the symmetry problem. Hence to break this symmetry the weights connected to the same neuron should not be initialized to the same value. Never initialize all the weights to zero. Never initialize all the weights to the same value.\n",
    "\n",
    "Now imagine that you initialize all weights to the same value (e.g. zero or one). In this case, each hidden unit will get exactly the same signal. E.g. if all weights are initialized to 1, each unit gets signal equal to sum of inputs (and outputs sigmoid(sum(inputs)) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb600ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2.\tIs it OK to initialize the bias terms to 0?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff007f",
   "metadata": {},
   "source": [
    "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights.\n",
    "\n",
    "Initializing all the weights with zeros leads the neurons to learn the same features during training. In fact, any constant initialization scheme will perform very poorly. Consider a neural network with two hidden units, and assume we initialize all the biases to 0 and the weights with some constant α.\n",
    "\n",
    "If you initialize all weights with zeros then every hidden unit will get zero independent of the input. So, when all the hidden neurons start with the zero weights, then all of them will follow the same gradient and for this reason \"it affects only the scale of the weight vector, not the direction\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef346813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3.\tName three advantages of the SELU activation function over ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4363b",
   "metadata": {},
   "source": [
    "RELU is clearly converging much faster than SELU .Still, RELU seems to be doing a much better job than SELU for the default configuration. This behavior remains more or less the same after iterating through hyperparameters. \n",
    "The activation of the SiLU is computed by the sigmoid function multiplied by its input and it looks like a continuous and “undershooting” version of the linear rectifier unit (ReLU) (Hahnloser et al., 2000). The activation of the dSiLU looks like steeper and “overshooting” version of the sigmoid function.\n",
    "\n",
    "ReLU6 is a modification of the rectified linear unit where we limit the activation to a maximum size of . This is due to increased robustness when used with low-precision computation. Image Credit: PyTorch. Source: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4001c488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4.\tIn which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c7162a",
   "metadata": {},
   "source": [
    "* SELU\n",
    "\n",
    "Scaled Exponential Linear Unit (SELU) SELU: A special case of normalization. Weight Initialization + Dropout. The Scaled Exponential Linear Unit (SELU) activation function is defined as: if x > 0: return scale * x. if x < 0: return scale * alpha * (exp(x) - 1). this activation functions that induce self-normalizing properties. The SELU activation function is given by. f ( x ) = λ x if x ≥ 0 f ( x ) = λ α ( exp ⁡ with α ≈ 1.6733 and λ ≈ 1.0507 .\n",
    "\n",
    "* leaky ReLU\n",
    "\n",
    "Leaky ReLU function is an improved version of the ReLU activation function. As for the ReLU activation function, the gradient is 0 for all the values of inputs that are less than zero, which would deactivate the neurons in that region and may cause dying ReLU problem.\n",
    "\n",
    "* ReLUs\n",
    "\n",
    "One reason you should consider when using ReLUs is, that they can produce dead neurons. That means that under certain circumstances your network can produce regions in which the network won't update, and the output is always 0.\n",
    "\n",
    "*  tanh\n",
    "\n",
    "The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1 to 1.\n",
    "\n",
    "* logistic\n",
    "\n",
    "It is commonly used for models where we have to predict the probability as an output. Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice because of its range.\n",
    "\n",
    "* softmax\n",
    "\n",
    "The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28317871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5.\tWhat may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ea4a6d1c",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)\n",
    "\n",
    "That sequence V is the one plotted yellow above. Beta (momentum hyperparameter)is another hyper-parameter which takes values from 0 to one. It is a good value and most often used in SGD with momentum. Intuitively, you can think of beta as follows. We’re approximately averaging over last 1 / (1- beta) points of sequence. Let’s see how the choice of beta affects our new sequence V.\n",
    "\n",
    "As you can see, with smaller numbers of beta(momentum hyperparameter), the new sequence turns out to be fluctuating a lot, because we’re averaging over smaller number of examples and therefore are ‘closer’ to the noisy data. With bigger values of beta(momentum hyperparameter), like beta=0.98, we get much smother curve, but it’s a little bit shifted to the right, because we average over larger number of example(around 50 for beta=0.98). Beta = 0.9 provides a good balance between these two extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fda272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6.\tName three ways you can produce a sparse model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b28fcdf",
   "metadata": {},
   "source": [
    "Sparse modeling is a component in many state of the art signal processing\n",
    "and machine learning tasks.\n",
    "\n",
    "• image processing (denoising, inpainting, superresolution)\n",
    "\n",
    "• Object recognition\n",
    "\n",
    "• general supervised learning\n",
    "\n",
    "• Building graphs for large scale semi-supervised learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70ea07e",
   "metadata": {},
   "source": [
    "Given a\n",
    "d\n",
    "×\n",
    "n matrix\n",
    "X of\n",
    "n points in\n",
    "R\n",
    "d\n",
    ".\n",
    "\n",
    "• Want to factor\n",
    "X\n",
    "≈ W Z, where\n",
    "W is\n",
    "d\n",
    "×\n",
    "K, and\n",
    "Z is\n",
    "K\n",
    "×\n",
    "N.\n",
    "\n",
    "•\n",
    "W is a dictionary,\n",
    "Z are the coefficients.\n",
    "\n",
    "• We need to choose an appropriate notion of “close” and conditions\n",
    "on\n",
    "Z to force the decomposition to be parsimonious\n",
    "\n",
    "• If we restrict the size of K < min(d, N), and “close” is operator or\n",
    "Frobenious norm, we get PCA.\n",
    "\n",
    "• If we restrict Zij ∈ {0, 1}, and Pi Zij = 1 (i.e. Zj is really sparse!), we\n",
    "get K-means\n",
    "\n",
    "• Everything in between (including the endpoints): dictionary learning.\n",
    "E.g.\n",
    "arg min\n",
    "Z∈RK×n,W∈S(d−1)×K X\n",
    "n\n",
    "j=1\n",
    "||W zj − xj||2, ||zj||0 ≤ q,\n",
    "\n",
    "• or the Z coordinate convexification:\n",
    "arg min\n",
    "Z∈RK×n,W∈S(d−1)×K Xj=1 ||W zj − xj||2 + λ||zj||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1bb329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929ec09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7.\tDoes dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab786a75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c48407e5",
   "metadata": {},
   "source": [
    "Logically, by omitting at each iteration neurons with a dropout, those omitted on an iteration are not updated during the backpropagation. They do not exist. So the training phase is slowed down. Applying dropout to the input layer increased the training time per epoch by about 25 %, independent of the dropout rate. Dropout has been widely used in deep learning to prevent overfitting. \n",
    "\n",
    "\n",
    "In the original implementation of dropout, dropout does work in both training time and inference time. During training time, dropout randomly sets node values to zero. In the original implementation, we have “keep probability” \n",
    "p\n",
    "keep\n",
    ". So dropout randomly kills node values with “dropout probability” \n",
    "1\n",
    "−\n",
    "p\n",
    "keep\n",
    ". During inference time, dropout does not kill node values, but all the weights in the layer were multiplied by \n",
    "p\n",
    "keep\n",
    ". One of the major motivations of doing so is to make sure that the distribution of the values after affine transformation during inference time is close to that during training time. Equivalently, This multiplier could be placed on the input values rather than the weights.\n",
    "\n",
    "Concretely, say we have a vector \n",
    "x\n",
    "=\n",
    "{\n",
    "1\n",
    ",\n",
    "2\n",
    ",\n",
    "3\n",
    ",\n",
    "4\n",
    ",\n",
    "5\n",
    "}\n",
    " as the input to certain fully connected layer and \n",
    "p\n",
    "keep\n",
    "=\n",
    "0.8\n",
    ". During training time, \n",
    "x\n",
    " could be set to \n",
    "{\n",
    "1\n",
    ",\n",
    "0\n",
    ",\n",
    "3\n",
    ",\n",
    "4\n",
    ",\n",
    "5\n",
    "}\n",
    " due to dropout. During inference time, \n",
    "x\n",
    " would be set to \n",
    "{\n",
    "0.8\n",
    ",\n",
    "1.6\n",
    ",\n",
    "2.4\n",
    ",\n",
    "3.2\n",
    ",\n",
    "4.0\n",
    "}\n",
    " while the weights remain unchanged.\n",
    " \n",
    " Things are similar if you placed the multiplier to the output values rather than the input values or weights. However, in any of the implementations mentioned above, you have to make changes to the values in the neural network during both training time and inference time. This seems to be undesirable to TensorFlow. So TensorFlow has its own implementation of dropout which only does work during training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ba8241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "092ed0a0",
   "metadata": {},
   "source": [
    "### 8.\tPractice training a deep neural network on the CIFAR10 image dataset:\n",
    "a.\tBuild a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
    "\n",
    "b.\tUsing Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
    "\n",
    "c.\tNow try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "d.\tTry replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "\n",
    "e.\tTry regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f85441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting class names\n",
    "class_names=[‘airplane’,’automobile’,’bird’,’cat’,’deer’,’dog’,’frog’,’horse’,’ship’,’truck’]\n",
    "x_train=x_train/255.0\n",
    "x_train.shape\n",
    "x_test=x_test/255.0\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c301e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_model=tf.keras.models.Sequential()\n",
    "# First Layer\n",
    "cifar10_model.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3, padding=”same”, activation=”relu”, input_shape=[32,32,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9b8757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MaxPoolingLayer\n",
    "cifar10_model.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2, padding=’valid’))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a4b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening Layer\n",
    "cifar10_model.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droput Layer\n",
    "cifar10_model.add(Dropout(0.2))\n",
    "# Adding the first fully connected layer\n",
    "cifar10_model.add(tf.keras.layers.Dense(units= 128,activation='relu’))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d847968",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
