{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1.\tWhy would you want to use the Data API?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768719e",
   "metadata": {},
   "source": [
    "API is an acronym for Application Programming Interface that software uses to access data, server software or other applications and have been around for quite some time. APIs are very versatile and can be used on web-based systems, operating systems, database systems and computer hardware.\n",
    "\n",
    "APIs are needed to bring applications together in order to perform a designed function built around sharing data and executing pre-defined processes. They work as the middle man, allowing developers to build new programmatic interactions between the various applications people and businesses use on a daily basis.\n",
    "\n",
    "The Data API can be enabled for Aurora Serverless DB clusters using specific Aurora MySQL and Aurora PostgreSQL versions only. For more information, see Data API for Aurora Serverless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea51090b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2.\tWhat are the benefits of splitting a large dataset into multiple files?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c528a90",
   "metadata": {},
   "source": [
    "Splitting a shared database can help improve its performance and reduce the chance of database file corruption. After you split database, you may decide to move the back-end database, or to use a different back-end database. You can use the Linked Table Manager to change the back-end database that you use.\n",
    "\n",
    "Even in non-relational databases, you’d want to have logically different data in different “buckets” in most situations. Putting your user list in the same table as your game equipment properties wouldn’t make much sense, even if these are stored in Cassandra or MongoDB and you aren’t going to use joins.\n",
    "\n",
    "Among other badness, even if you have a freeform name-value-pair storage scheme or giant list of JSONs, sticking everything in one database “table-thing” will impair search and lookup performance and make maintaining your data more tedious and difficult.\n",
    "\n",
    "In a relational database, you almost never want generic “dumping ground” tables, as this will invariably cause problems with Database normalization, performance, and ongoing database maintenance.\n",
    "\n",
    "Occasionally you’ll have more “freeform” tables that may have a few property columns and one or more “blobby” VARCHARs or TEXT fields - common use-cases for these types of tables are log tables, user activity tables, or audit tables, but even these should be normalized and use other tables for things they routinely reference, such as users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a0820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3.\tDuring training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971b5f51",
   "metadata": {},
   "source": [
    "Identifying the Bottleneck\n",
    "There are a number of different tools and techniques for evaluating the runtime performance of a training session, and identifying and studying an input pipeline bottleneck. Let’s review just a few of them:\n",
    "\n",
    "1. System Metrics\n",
    "\n",
    "2. Performance Profilers\n",
    "\n",
    "3. Throughput Measurement\n",
    "\n",
    "four steps for addressing the preprocessing data bottleneck.\n",
    "\n",
    "1. Identify any operations that can be moved to the data preparation phase\n",
    "\n",
    "2. Optimize the data pre-processing pipeline\n",
    "\n",
    "3. Perform some of the pre-processing steps on the GPU\n",
    "\n",
    "4. Use the TensorFlow data service to offload some of the CPU compute to other machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fac7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4.\tCan you save any binary data to a TFRecord file, or only serialized protocol buffers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85e73f7",
   "metadata": {},
   "source": [
    "The TFRecord format is a simple format for storing a sequence of binary records. Protocol buffers are a cross-platform, cross-language library for efficient serialization of structured data. Protocol messages are defined by . proto files, these are often the easiest way to understand a message type.\n",
    "\n",
    "The High Resolution Rapid Refresh (HRRR) model is a numerical weather model. Because weather models work best when countries all over the world pool their observations, the format for weather data is decided by the World Meteorological Organization and it is super-hard to change. So, the HRRR data is disseminated in a #@!$@&= binary format called GRIB.\n",
    "Regardless of the industry you are in — manufacturing, electricity generation, pharmaceutical research, genomics, astronomy— you probably have some format like this. A format that no modern software framework supports. Even though this article is about HRRR, the techniques here will apply to any binary files you have.\n",
    "The most efficient format for TensorFlow training is TensorFlow records. This is a protobuf format that makes it possible for the training program to buffer, prefetch, and parallelize the reading of records. So, a good first step for machine learning is to convert your industry-specific binary format files into TensorFlow records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f2e70f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5.\tWhy would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed30053",
   "metadata": {},
   "source": [
    "Protocol buffers, or Protobuf, is a binary format created by Google to serialize data between different services. Google made this protocol open source and now it provides support, out of the box, to the most common languages, like JavaScript, Java, C#, Ruby and others\n",
    "\n",
    "When a message is encoded, the keys and values are concatenated into a byte stream. When the message is being decoded, the parser needs to be able to skip fields that it doesn't recognize. This way, new fields can be added to a message without breaking old programs that do not know about them.\n",
    "\n",
    "The main problem with protobuf for large files is that it doesn't support random access. You'll have to read the whole file, even if you only want to access a specific item. If your application will be reading the whole file to memory anyway, this is not an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb02fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6.\tWhen using TFRecords, when would you want to activate compression? Why not do it systematically?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c3984",
   "metadata": {},
   "source": [
    "The TFRecord format is a simple format for storing a sequence of binary records. Converting your data into TFRecord has many advantages, such as:\n",
    "\n",
    "* More efficient storage: the TFRecord data can take up less space than the original data; it can also be partitioned into multiple files.\n",
    "\n",
    "* Fast I/O: the TFRecord format can be read with parallel I/O operations, which is useful for TPUs or multiple hosts.\n",
    "* Self-contained files: the TFRecord data can be read from a single source—for example, the COCO2017 dataset originally stores data in two folders (\"images\" and \"annotations\").\n",
    "\n",
    "An important use case of the TFRecord data format is training on TPUs. First, TPUs are fast enough to benefit from optimized I/O operations. In addition, TPUs require data to be stored remotely (e.g. on Google Cloud Storage) and using the TFRecord format makes it easier to load the data without batch-downloading.\n",
    "\n",
    "Performance using the TFRecord format can be further improved if you also use it with the tf.data API.\n",
    "\n",
    "In this example you will learn how to convert data of different types (image, text, and numeric) into TFRecord.\n",
    "Reading compressed input is implemented in TensorFlow, but supporting outputting compressed TFRecord files would be amazing, as TFRecord is a rather inefficient format in terms of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d01f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6950b15f",
   "metadata": {},
   "source": [
    "### 7.\tData can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96872f9",
   "metadata": {},
   "source": [
    "Data Preprocessing is carried out to remove the cause of unformatted real-world data which we discussed above. First of all, let's explain how missing data can be handled during Data Preparation. Three different steps can be executed which are given below -\n",
    "\n",
    "* Ignoring the missing record - It is the simplest and efficient method for handling the missing data. But, this method should not be performed at the time when the number of missing values is immense or when the pattern of data is related to the unrecognized primary root of the cause of the statement problem.\n",
    "\n",
    "* Filling the missing values manually - This is one of the best-chosen methods of Data Preparation process. But there is one limitation that when there are large data set, and missing values are significant then, this approach is not efficient as it becomes a time-consuming task.\n",
    "\n",
    "* Filling using computed values - The missing values can also be occupied by computing mean, mode or median of the observed given values. Another method could be the predictive values in Data Preprocessing is that are computed by using any Machine Learning or Deep Learning tools and algorithms. But one drawback of this approach is that it can generate bias within the data as the calculated values are not accurate concerning the observed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd2a03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
